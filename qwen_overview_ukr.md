# Огляд китайських моделей ШІ від Alibaba, Baidu та DeepSeek
## Qwen AI

![Qwen](https://upload.wikimedia.org/wikipedia/commons/thumb/6/69/Qwen_logo.svg/960px-Qwen_logo.svg.png)
Qwen (спочатку запущена в Китаї як Tongyi Qianwen) – це серія великих мовних модель та мультимодальних моделей команди Qwen, Alibaba Group. Як мовні, так і мультимодальні моделі попередньо навчаються на великомасштабних багатомовних та мультимодальних даних, а потім навчаються на якісних даних для узгодження з людськими вподобаннями. Qwen здатний розуміти природну мову, генерувати текст, розуміти зір, розуміти звук, використовувати інструменти, грати ролі, грати роль агента штучного інтелекту тощо.​[[1]](https://qwen.ai/qwenchat) Багато варіантів випускаються за ліцензіями, що дозволяють розробникам завантажувати, запускати та налаштовувати їх на власній інфраструктурі.[[2]](https://en.wikipedia.org/wiki/Qwen) Вони пропонують унікальне поєднання продуктивності, гнучкості та відкритості, що робить їх ідеальними як для підприємств, так і для дослідницьких застосувань. Їхня швидка еволюція дозволила їм залишатися на передовій розробки LLM.

### Ключові можливості
Моделі Qwen можуть:
* Розуміти та створювати багатомовний текст для таких завдань, як чат, переклад, резюме та письмові роботи у довгих формах.[[3]](https://qwen.readthedocs.io/)
* Виконувати мультимодальне мислення, включаючи розуміння зображень та звуків, генерацію зображень, а в новіших версіях – завдання, пов'язані з відео.[[4]](https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&from=research.latest-advancements-list)
* Підтримка довгоконтекстних вхідних даних (від десятків тисяч до понад 100 000 токенів), що допомагає в аналізі великих документів та тривалих розмов.[[5]](https://www.godofprompt.ai/blog/what-is-qwen-ai)

### Можливості та інтерфейси
Qwen Chat – це основний інтерфейс для користувача, який пропонує взаємодію в стилі чат-бота, а також такі функції, як аналіз документів, інтеграція веб-пошуку та інструменти для творчого письма. Сторонні платформи та інструменти також надають доступ до моделей Qwen через API або розміщені ігрові майданчики, орієнтуючись на компанії, розробників та дослідників, яким потрібен просунутий механізм штучного інтелекту.[[6]](https://www.prismetric.com/qwen-2-5-what-it-is-and-how-to-use-it/)

## Еволюція Qwen: Від Qwen 1 до Qwen 3 
### Qwen 1 & Qwen 1.5
* Початкові релізи були зосереджені на надійних архітектурах трансформаторів та багатомовних можливостях.[[7]](https://datasciencedojo.com/blog/the-evolution-of-qwen-models/)
* Перші публічні моделі Qwen включали розміри параметрів: 1,8B, 7B, 14B та 72B.[[8]](https://github.com/QwenLM/Qwen)
* Контекстні вікна до 32 тисяч токенів.
* Існували як «base» моделі (попередньо навчені для загального тексту), так і варіанти «Chat» / налаштовані на інструкції (для розмови, використання інструментів, генерації тощо).[[9]](https://github.com/QwenLM/Qwen)
* Згідно з оригінальним технічним звітом, Qwen (v1) підтримував «розуміння природної мови, генерацію тексту, використання інструментів, генерацію коду / міркування, математику, багатомовний текст» — навіть варіанти, спеціалізовані на коді та математиці (наприклад, «Code-Qwen», «Math-Qwen»).[[10]](https://skywork.ai/blog/qwen-tongyi-qianwen-open-weight-ai-model)
* Ефективність завдяки MoE та квантуванню: варіант MoE (наприклад, Qwen1.5-MoE-A2.7B) дозволяє користувачам отримати «продуктивність великої моделі з меншим використанням ресурсів», що допомагає при розгортанні на обмеженому обладнанні.[[11]](https://www.reddit.com/r/machinelearningnews/comments/1br5kqa)
* Висока продуктивність з китайською та англійською мовами, зі зростанням підтримки інших мов

#### Список моделей Qwen 1.5[[12]](https://ollama.com/library/qwen)

| Серія   | Модель    | Розмір | Контекст | Ввід  |
| ------- | --------- | ------ | -------- | ----- |
| Qwen1.5 | Qwen-0.5B | 0.5B   | 32K      | Текст |
|         | Qwen-1.8B | 1.8B   | 32K      | Текст |
|         | Qwen-4B   | 4B     | 32K      | Текст |
|         | Qwen-7B   | 7B     | 32K      | Текст |
|         | Qwen-14B  | 14B    | 32K      | Текст |
|         | Qwen-32B  | 32B    | 32K      | Текст |
|         | Qwen-72B  | 72B    | 32K      | Текст |
|         | Qwen-110B | 100B   | 32K      | Текст |

### Qwen 2 & Qwen 2.5
* Розширені розміри параметрів (110B dense, 72B instruct).[[13]](https://datasciencedojo.com/blog/the-evolution-of-qwen-models/)
* Покращені навчальні данні (до 18 трильйонів токенів у Qwen 2.5).
* Покращене узгодження за допомогою контрольованого точного налаштування та Direct Preference Optimization(DPO).
* Спеціалізовані моделі для математичних, кодувальних та візуально-мовних завдань.
* Усі варіанти мають архітектуру Transformer-декодера, використовуючи сучасні компоненти (наприклад, активації SwiGLU, попередню нормалізацію за допомогою RMSNorm) для стабільності навчання.[[14]](https://qwen-ai.chat/models/qwen2/?utm_source=chatgpt.com)
* Він пропонує потужну багатомовну підтримку, довгі контекстні вікна до приблизно 128–131 тисяч токенів з використанням подвійної уваги до фрагментів та масштабування RoPE/YARN, а також конкурентоспроможну продуктивність у тестах на мову, кодування, математику та міркування порівняно з попередніми моделями відкритої ваги.​[[15]](https://www.emergentmind.com/topics/qwen-series-models)
* Як Qwen2, так і Qwen2.5 використовують сучасні варіанти дизайну трансформаторів: згруповану увагу до запитів, позиційне кодування RoPE, RMSNorm та SwiGLU, а також шаблони ковзного вікна та повної уваги для ефективності в довгому контексті.[[16]](https://huggingface.co/docs/transformers/en/model_doc/qwen2)

#### Список моделей Qwen 2[[17]](https://ollama.com/library/qwen2)

| Серія | Модель     | Розмір | Контекст | Ввід  |
| ----- | ---------- | ------ | -------- | ----- |
| Qwen2 | Qwen2-0.5B | 0.5B   | 32K      | Текст |
|       | Qwen2-1.5B | 1.5B   | 32K      | Текст |
|       | Qwen2-7B   | 7B     | 128K     | Текст |
|       | Qwen2-72B  | 72B    | 128K     | Текст |



Qwen2 навчений на даних 29 мов, включаючи англійську та китайську. Він доступний з 4 розмірами параметрів: 0.5B, 1.5B, 7B, 72B. У моделях 7B та 72B довжину контексту було збільшено до 128тис. токенів.

| Моделі         | Qwen2-0.5B | Qwen2-1.5B | Qwen2-7B | Qwen2-72B |
|----------------|------------|------------|----------|-----------|
| Params         | 0.49B      | 1.54B      | 7.07B    | 72.71B    |
| Non-Emb Params | 0.35B      | 1.31B      | 5.98B    | 70.21B    |
| GQA            | True       | True       | True     | True      |
| Tie Embedding  | True       | True       | False    | False     |
| Context Length | 32K        | 32K        | 128K     | 128K      |

#### Підтримка мов
| Регіон                     | Мови                                                                                                      |
| -------------------------- | --------------------------------------------------------------------------------------------------------- |
| Західна Європа             | Німецька, Французька, Іспанська, Португальська, Італійська, Нідерландська                                 |
| Східна й Центральна Європа | Чеська, Полська, Російська                                                                                |
| Близький Схід              | Арабська, Перська, Іврит, Турецька                                                                        |
| Східна Азія                | Японська, Корейська                                                                                       |
| Південно-Східна Азія       | В'єтнамська, Тайландська, Індонезійська, Малайзійська, Лаоська, Бірманська, Себуано, Кхмерська, Тагальска |
| Південна Азія              | Хінді, Бенгальська, Урду                                                                                  |

#### Продуктивність Qwen2
![Qwen2-72B](https://ollama.com/assets/library/qwen2/68b445e3-bf1b-4fff-9621-4e5bbf4a72a2)
![Qwen2-72Bins](https://ollama.com/assets/library/qwen2/72e9bf41-f8d6-4b7a-a7ef-9599ef533af6)
![Qwen2-7Bins](https://ollama.com/assets/library/qwen2/6c978d72-c37c-45a2-b7f4-c06178c0182c)

#### Список моделей Qwen 2.5[[18]](https://ollama.com/library/qwen2.5)
| Серія   | Модель       | Розмір | Контекст | Ввід  |
| ------- | ------------ | ------ | -------- | ----- |
| Qwen2.5 | Qwen2.5-0.5B | 0.5B   | 32K      | Текст |
|         | Qwen2.5-1.5B | 1.5B   | 32K      | Текст |
|         | Qwen2.5-3B   | 3B     | 32K      | Текст |
|         | Qwen2.5-7B   | 7B     | 32K      | Текст |
|         | Qwen2.5-14B  | 14B    | 32K      | Текст |
|         | Qwen2.5-32B  | 32B    | 32K      | Текст |
|         | Qwen2.5-72B  | 72B    | 32K      | Текст |

### Qwen 3
* Випущена у 2025 році, Qwen 3 знаменує собою стрибок в архітектурі, масштабі та міркуванні.
* Qwen3 пропонує два внутрішні режими роботи: швидкий режим «без мислення» для повсякденних розмов і повільніший режим «обдумування», який виділяє більше обчислювальних ресурсів для складних міркувань, математики та кодування, які можна вибрати за допомогою підказок або налаштувань API.[[19]](https://qwenlm.github.io/blog/qwen3/)
* Сімейство охоплює компактні моделі (наприклад, з параметрами близько 0,6B, 1,7B, 4B, 8B, 14B, 32B) та варіанти MoE, такі як Qwen3‑30B‑A3B та Qwen3‑235B‑A22B, де для високої ефективності на кожен токен активується лише підмножина експертів.[[20]](https://www.siliconflow.com/articles/en/the-best-qwen3-models-in-2025)
* Qwen3 розширює багатомовне покриття з кількох десятків мов у Qwen2.5 приблизно до 100–119 мов та діалектів, зі значно покращеним міжмовним розумінням та якістю перекладу.[[21]](https://qwenlm.github.io/blog/qwen3/)

#### Список моделей Qwen 3[[22]](https://ollama.com/library/qwen3)

Qwen3 — це останнє покоління великих мовних моделей у серії Qwen, що пропонує повний набір щільних та змішаних експертних (MoE) моделей. Флагманська модель, Qwen3-235B-A22B, досягає конкурентних результатів у бенчмарк-оцінках кодування, математики, загальних можливостей тощо порівняно з іншими моделями вищого рівня, такими як DeepSeek-R1, o1, o3-mini, Grok-3 та Gemini-2.5-Pro. Крім того, мала модель MoE, Qwen3-30B-A3B, перевершує QwQ-32B за кількістю активованих параметрів у 10 разів, і навіть така крихітна модель, як Qwen3-4B, може конкурувати за продуктивністю з Qwen2.5-72B-Instruct.

| Серія | Модель     | Розмір | Контекст | Ввід  |
| ----- | ---------- | ------ | -------- | ----- |
| Qwen3 | Qwen3-0.6B | 0.6B   | 40K      | Текст |
|       | Qwen3-1.7B | 1.7B   | 40K      | Текст |
|       | Qwen3-4B   | 4B     | 256K     | Текст |
|       | Qwen3-8B   | 8B     | 40K      | Текст |
|       | Qwen3-14B  | 14B    | 40K      | Текст |
|       | Qwen3-30B  | 30B    | 256K     | Текст |
|       | Qwen3-32B  | 32B    | 40K      | Текст |
|       | Qwen3-235B | 235B   | 256K     | Текст |


#### Продуктивність Qwen3
![Qwen3-30B](https://ollama.com/assets/library/qwen3/bc0ddfea-95b5-49fc-a36e-c817f98a5de0)
![Qwen3-235B](https://ollama.com/assets/library/qwen3/8426a459-dd88-49cd-ae89-ece442e58ec5)

#### Список моделей Qwen 3-VL[[23]](https://ollama.com/library/qwen3-vl)

Qwen3-VL – це найпотужніша на сьогодні візуальна модель в родині Qwen.

У цьому поколінні модель покращилася в багатьох сферах: розуміння та генерація тексту, сприйняття та міркування про візуальний контент, підтримка довших контекстних фрагментів, розуміння просторових зв'язків та динамічних відео, а також взаємодія з агентами штучного інтелекту — Qwen3-VL демонструє чіткий та значний прогрес у кожній галузі.

| Серія    | Модель        | Розмір | Контекст | Ввід              |
| -------- | ------------- | ------ | -------- | ----------------- |
| Qwen3-VL | Qwen3-VL-2B   | 2B     | 256K     | Текст, Зображення |
|          | Qwen3-VL-4B   | 4B     | 256K     | Текст, Зображення |
|          | Qwen3-VL-8B   | 8B     | 256K     | Текст, Зображення |
|          | Qwen3-VL-30B  | 30B    | 256K     | Текст, Зображення |
|          | Qwen3-VL-32B  | 32B    | 256K     | Текст, Зображення |
|          | Qwen3-VL-235B | 235B   | 256K     | Текст, Зображення |


#### Продуктивність Qwen3-VL
![Qwen3-VL-235Bins](https://camo.githubusercontent.com/7b30ae6af5402a71bc63076de6e1b67d5bf94c1ada1c3fb59fddaf30b98fbd8a/68747470733a2f2f7169616e77656e2d7265732e6f73732d616363656c65726174652e616c6979756e63732e636f6d2f5177656e332d564c2f7461626c655f6e6f7468696e6b696e675f766c2e6a7067)
![Qwen3-VL-235Bth](https://camo.githubusercontent.com/33b15065d776936438f83ec82199597884a776239ef6318ee82fe175bb0505f0/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d564c2f7461626c655f7468696e6b696e675f766c5f2e6a7067)

#### Список моделей Qwen 3-Coder[[24]](https://ollama.com/library/qwen3-coder)

Qwen3-Coder — це продуктивні моделі довгого контексту від Alibaba для агентних та завдань програмування.

| Серія       | Модель           | Розмір | Контекст | Ввід  |
| ----------- | ---------------- | ------ | -------- | ----- |
| Qwen3-Coder | Qwen3-Coder-30B  | 30B    | 256K     | Текст |
|             | Qwen3-Coder-480B | 480B   | 256K     | Текст |

#### Продуктивність Qwen3-Coder
![Qwen3-Coder](https://ollama.com/assets/library/qwen3-coder/52070971-5a66-4947-90a0-5e983a5809e7)

#### Моделі Qwen 3-Embedding[[25]](https://ollama.com/library/qwen3-embedding)

Серія моделей Qwen3-Embedding спеціально розроблена для завдань вбудовування тексту. Спираючись на щільні базові моделі серії Qwen3, вона пропонує повний спектр моделей вбудовування тексту різних розмірів (0.6B, 4B та 8B). Ця серія успадковує виняткові багатомовні можливості, розуміння довгих текстів та навички міркування своєї базової моделі. Серія Qwen3 Embedding являє собою значний прогрес у задачах вбудовування та ранжування кількох текстів, включаючи пошук тексту, пошук коду, класифікацію тексту, кластеризацію тексту та бітекстовий аналіз.

Qwen3-Embedding-8B має такі можливості[[26]](https://qwenlm.github.io/blog/qwen3-embedding/):
* Тип моделі: Вбудовування тексту
* Підтримка мов: 100+ мов
* Кількість параметрів: 8B
* Контекст: 32k
* Розмір вбудовування: До 4096, підтримує визначені користувачем вихідні розміри в діапазоні від 32 до 4096

| Серія           | Модель               | Розмір | Контекст | Ввід | Вбудовування | Підтримка MRL | Розуміння інструкцій |
| --------------- | -------------------- | ------ | -------- | ---- | ------------ | ------------- | -------------------- |
| Qwen3-Embedding | Qwen3-Embedding-0.6B | 0.6B   | 32K      | Text | 1024         | Yes           | Yes                  |
|                 | Qwen3-Embedding-4B   | 4B     | 32K      | Text | 2560         | Yes           | Yes                  |
|                 | Qwen3-Embedding-8B   | 8B     | 32K      | Text | 4096         | Yes           | Yes                  |
| Qwen3-Reranking | Qwen3-Reranking-0.6B | 0.6B   | 32K      | Text | -            | -             | Yes                  |
|                 | Qwen3-Reranking-4B   | 4B     | 32K      | Text | -            | -             | Yes                  |
|                 | Qwen3-Reranking-8B   | 8B     | 32K      | Text | -            | -             | Yes                  |


#### Продуктивність моделей Reranker
| Model                              | Param | MTEB-R    | CMTEB-R   | MMTEB-R   | MLDR      | MTEB-Code | FollowIR  |
|------------------------------------|-------|-----------|-----------|-----------|-----------|-----------|-----------|
| Qwen3-Embedding-0.6B               | 0.6B  | 61.82     | 71.02     | 64.64     | 50.26     | 75.41     | 5.09      |
| Jina-multilingual-reranker-v2-base | 0.3B  | 58.22     | 63.37     | 63.73     | 39.66     | 58.98     | -0.68     |
| gte-multilingual-reraner-base      | 0.3B  | 59.51     | 74.08     | 59.44     | 66.33     | 54.18     | -1.64     |
| BGE-reranker-v2-m3                 | 0.6B  | 57.03     | 72.16     | 58.36     | 59.51     | 41.38     | -0.01     |
| Qwen3-Reranker-0.6B                | 0.6B  | 65.80     | 71.31     | 66.36     | 67.28     | 73.42     | 5.41      |
| Qwen3-Reranker-4B                  | 4B    | **69.76** | 75.94     | 72.74     | 69.97     | 81.20     | **14.84** |
| Qwen3-Reranker-8B                  | 8B    | 69.02     | **77.45** | **72.94** | **70.19** | **81.22** | 8.05      |